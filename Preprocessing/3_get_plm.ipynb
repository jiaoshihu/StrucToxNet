{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22bafee8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d9f3611",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_fasta_file(filepath):\n",
    "    with open(filepath, \"r\") as file:  \n",
    "        lines = [line.strip() for line in file]\n",
    "        sequences_dict = {lines[i][1:]: lines[i+1] for i in range(0, len(lines), 2)}\n",
    "    return sequences_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4992ce3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_sequences = read_fasta_file('./inputs/test_sequences.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "052a4e63",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/jiaoshihu/toolkits/PLM/prot_t5_xl_uniref50\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path, do_lower_case=False)\n",
    "model = T5EncoderModel.from_pretrained(model_path)\n",
    "model = model.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e697abc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_plm_representation(sequences_dict):\n",
    "    representations = {}\n",
    "    peptide_names = sequences_dict.keys()\n",
    "    for name in peptide_names:\n",
    "        seq = sequences_dict[name]\n",
    "        seq = [' '.join(seq)]\n",
    "        ids = tokenizer(seq, add_special_tokens=True, padding=True, return_tensors='pt')\n",
    "        \n",
    "        input_ids = ids['input_ids'].clone().detach().to('cuda')\n",
    "        attention_mask = ids['attention_mask'].clone().detach().to('cuda')\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embedding_repr = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        embedding = embedding_repr.last_hidden_state.cpu().numpy()\n",
    "\n",
    "        \n",
    "        seq_len = (attention_mask == 1).sum()\n",
    "        seq_emd = embedding[0, :seq_len-1, :]\n",
    "        \n",
    "        representations[name] = seq_emd\n",
    "        \n",
    "    return representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35c6b886",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_representation = get_plm_representation(test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf6e4934",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def nom_representation(origin_representation, outfile):\n",
    "    x_max = np.load('./inputs/x_max.npy')\n",
    "    x_min = np.load('./inputs/x_min.npy')\n",
    "    \n",
    "    x_range = x_max - x_min\n",
    "    x_range[x_range == 0] = 1 \n",
    "    \n",
    "    normalized_representations = {}\n",
    "    for name, embeddings in origin_representation.items():\n",
    "        normalized_representations[name] = (embeddings - x_min) / x_range\n",
    "        \n",
    "    with open(outfile, 'wb') as file:\n",
    "        pickle.dump(normalized_representations, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47f79e39",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nom_representation(test_representation, './outputs/test_plm_representation.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}